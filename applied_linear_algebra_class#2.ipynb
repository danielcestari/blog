{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula #2\n",
    "\n",
    "Recapitulando a aula passada...\n",
    "\n",
    "#### Decomposição LU \n",
    "\n",
    "$\\forall A_{m \\times n}, A=LU$, $\\exists ! L \\text{ triangular inferior com } diag(L)=1$, $\\exists ! U \\text{ triangular superior }$, contanto que $det(A_i) \\neq 0, \\forall i=1..m-1$, sendo $A_i$ as submatrizes principais de $A$.\n",
    "\n",
    "Atenção, aqui o símbolo $!$ significa único(a), então $\\exists ! L$ significa que existe uma matriz L única.\n",
    "\n",
    "É possível perceber um problema com a maneira que o teorema da decomposição LU está definido, pois se o elemento $a_{11}=0$, $det(A_1)=0$ e todo o processo da decomposição nem começa. Mas se fizermos uma troca de linhas pode ser que consigamos encontrar a decomposição LU de tal matriz.\n",
    "\n",
    "Exemplo:\n",
    "$A = \\begin{bmatrix}\n",
    "    0 & 1 & 2  \\\\\n",
    "    1 & -2 & 2 \\\\\n",
    "    3 & -1 & 1 \n",
    "\\end{bmatrix}$ tem $det(A_1)=0$, $det(A_2) \\neq 0$ e $det(A)=det(A_3) \\neq 0$, logo não podemos aplicar o teorema nessa matriz. Mas a matriz:\n",
    "\n",
    "$A' = \\begin{bmatrix}\n",
    "    1 & -2 & 2 \\\\\n",
    "    0 & 1 & 2  \\\\\n",
    "    3 & -1 & 1 \n",
    "\\end{bmatrix}$, obtida trocando a primeira e segunda linhas, tem $det(A_1), det(A_2), det(A_3) \\neq 0$ logo podemos aplicar o teorema nessa nova matriz.\n",
    "\n",
    "A matriz $A'$ pôde ser obtida através da multiplicação de uma matriz de permutação $P$ na matriz original $A$. $A'=PA$, com $P = \\begin{bmatrix}\n",
    "    0 & 1 & 0  \\\\\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 0 & 1 \n",
    "\\end{bmatrix}$. Logo, a pergunta que fazemos, é sempre possível encontrar a decomposição LU da uma matriz $A'$ obtida pela permutação de outra que não respeita todas as condições do teorema? ($LU=A'=PA?$)\n",
    "\n",
    "Veremos que sim, e definiremos um novo teorema para a decomposição LU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposição LU Generalizada\n",
    "\n",
    "**Teorema:** Toda matriz tem uma decomposição LU se for permitido fazermos troca de linhas.\n",
    "\n",
    "$PA=LU$, onde $P$ é uma matriz de permutação, ou seja, primeiro transformamos $A$ em $A'$, que agora satisfaz as hipóteses do teorema, e então fazemos a decomposição LU.\n",
    "\n",
    "A matriz $P$ não é computada de antemão, mas conforme o processo da decomposição progride, pois sua determinação de antemão seria muito custosa, teríamos que verificar que todos as submatrizes principais têm determinante não nulo. Ao invés disso, conforme o processo da decomposição vai sendo iterado, se em algum momento $k$ o determinando da submatriz for nulo é feita a troca das linhas $k$ e $k+1$, e continua-se o processo. Se a troca das linhas $k$ pela $k+1$ ainda produzir um determinante nulo, é feita a troca das linhas $k$ e $k+2$, e assim por diante até o determinante não se mais nulo. Caso todas as trocas de linhas resultarem em determiante nulo de uma submatriz principal, significa que a matriz não satisfaz as hipóteses da decomposição LU, e não há uma permutação que a transforme numa matriz que satisfaça.\n",
    "\n",
    "Lembre-se que eu falei que a prova por indução também dá um algoritmo para a decomposição? Então, é durante esse algoritmo conforme aumenta-se a matriz sendo decomposta, primeiro uma linha, depois duas, e assim por diante, que é é feita essa troca de linhas se necessário. Pelo menos conceitualmente como prova dessa decomposição generalizada. Eu digo conceitualmente pois esse pode não ser a forma mais eficience de implementar a decomposição. Na verdade veremos que essa decomposição é implementada pela execução do método de eliminação de Gauss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Essa ideia dos determinantes de todas as submatrizes principais serem não nulo nos leva ao conceito de matriz definida positiva (do inglês *positive definite* PD).\n",
    "\n",
    "Uma matriz $A$ é dita ser definida positiva se $A$ for simétrica e $det(A_k) > 0, \\forall k=1..n$. \n",
    "\n",
    "Outra definição é: $A$ é definida positiva se $(Ax,x) > 0, \\forall x \\neq 0$, $(Ax,x)= \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j$, $(Ax,x)$ é a notação para o produto interno de $Ax$ por x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Decomposição Cholesky\n",
    "O próximo teorema a ser apresentado é o da decomposição Cholesky\n",
    "\n",
    "**Teorema:** Uma matirz simétrica definida positiva (do inglês symmetric positive definite, SPD) pode ser escrita como $A=G^tG$, send $G$ triangular superior.\n",
    "\n",
    "A vantagem dessa decomposição em relação à LU é que o custo computacional da decmposição Cholesky (~$1/3 n^3$) é da ordem de metada do custo computacional da LU (~$2/3n^3$), pois na Cholesky só é preciso computar uma matriz, $G$, enquanto que na decomposição LU é necessário o cálculo de duas matrizes.\n",
    "\n",
    "Só de escrevermos a matriz $A$ como a multiplicação de $G^t$ por $G$ já é possível encontrar os coeficientes de $G$.\n",
    "\n",
    "$$\\left[\n",
    "    \\begin{array}{cccc|c}\n",
    "    a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & \\dots & a_{nn}\n",
    "    \\end{array}\n",
    "    \\right] = \\left[\n",
    "    \\begin{array}{cccc|c}\n",
    "    g_{11} & 0 & \\dots & 0 \\\\\n",
    "    g_{21} & g_{22} & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    g_{n1} & g_{n2} & \\dots & g_{nn}\n",
    "    \\end{array}\n",
    "    \\right] \\left[\n",
    "    \\begin{array}{cccc|c}\n",
    "    g_{11} & g_{12} & \\dots & g_{1n} \\\\\n",
    "    0 & g_{22} & \\dots & g_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & g_{nn}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "Multiplicando a primeira linha de $G^t$ pela primeira coluna de $G$, temos:\n",
    "\n",
    "$\n",
    "g_{11}.g_{11} = a_{11} \\implies g_{11} = \\sqrt{a_{11}}\n",
    "$, essa expressão é válida pois $A$ é PD $\\implies det(A_k) >0 \\implies det(A_1) > 0 \\implies a_{11} > 0$, logo a raíz de $a_{11}$ é bem definida.\n",
    "\n",
    "$$g_{11}.g_{1j}=a_{1j} \\implies g_{1j}=\\frac{a_{1j}}{g_{11}}, \\text{ já temos uma expressão para os coeficientes da primeira linha de }G.$$\n",
    "\n",
    "Para a próxima linha precisamos encontrar $g_{22}$. Então fazemos a multiplicação da segunda linha de $G^t$ pela segunda coluna de $G$. \n",
    "\n",
    "$$\n",
    "g_{12}^2 + g_{22}^2 = a_{22}, \\\\\n",
    "g_{22}^2 = a_{22} - g_{12}^2, \\text{ e como $A$ é PD, } g_{22} > 0, \\text{ pois } \\\\\n",
    "g_{22}^2 = a_{22} - \\Big(\\frac{a_{12}}{g_{11}}\\Big)^2 = \\frac{a_{22}g_{11}^2 - a_{12}^2}{g_{11}^2} = \\frac{a_{22}a_{11}-a_{12}^2}{a_{11}} > 0, \\text{ pois $A$ PD } \\implies det(A_2) > 0 \\text{ e } det(A_2) = a_{22}a_{11}-a_{12}^2, a_{12} = a_{21} \\text{ pois $A$ também é simétrica, e } det(A_1) = a_{11} > 0. \\\\\n",
    "\\text{Logo } g_{22} = \\sqrt{a_{22} - g_{12}^2} \\text{ está bem definida.}\n",
    "$$\n",
    "\n",
    "Seguindo multiplicando a segunda linha de $G^t$ pelas demais colunas de $G$ as única icógnitas serão os coeficientes $g_{2j}$, pois a primeira linha já foi calculada, e para cada multiplicação da segunda linha por outra coluna, haverá apenas uma icógnita.\n",
    "\n",
    "Esse processo segue até todos os coeficientes não nulos de $G$ sejam computados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Decomposição QR\n",
    "A decomposição QR é outra decomposição importante, ela é usada no cálculo dos autovalorese autovetores de uma matriz.\n",
    "\n",
    "**Teorema:** Toda matriz $A_{m \\times n}$ pode ser escrita como o produto de uma matriz unitária $Q$ e uma triangular superior $R$, ou seja, $A=QR$, onde $Q$ é unitária e $R$ é triangular superior. Por unitária, significa que $Q^HQ=I$, ($Q^H = \\overline{Q}^{\\, t}$, $Q^H$ é a transposta conjugada de $Q$), no caso de matrizes reais, untirária significa que $Q^tQ=I$, ou seja, $Q$ é ortogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Ortonormalização de Gram-Schmidt\n",
    "\n",
    "Dados vetores $u_1, u_2, ..., u_n \\in \\mathbb{R}^m$ LI sempre é possível construir a partir dos vetores dados, outros m vetores LI e ortonormais. Esse processo é a ortonormalização de Gram-Schmidt.\n",
    "O processo segue os seguintes passos.\n",
    "\n",
    "Começamos por um vetor $u_1$, e definimos $v_1$.\n",
    "\n",
    "$$ v_1 = \\frac{u_1}{ \\| u_1 \\|_2}, \\| u \\|_2 = \\sqrt{<u,u>} $$\n",
    "\n",
    "Depois escolhemos outro vetor $u_2$ e construímos $\\tilde{v}_2$ removendo a contribuição dos vetores $v_i$ anteriormente criados de $u_2$ de forma que $\\tilde{v}_2$ seja ortogonal, depois o normalizamos e produzimos $v_2$.\n",
    "\n",
    "$$ \\tilde{v}_2 = u_2 - \\alpha v_1 \\implies 0 = <\\tilde{v}_2, v_1> = <u_2- \\alpha u_1, v_1>  = <u-2, v_1> - \\alpha <v_1, v_1> = 0 \\\\\n",
    "\\alpha = \\frac{<u_2, v_1>}{<v_1, v_1>} = <u_2, v_1> \\text{, pois } <v_1, v_1>=1 \\\\\n",
    "v_2 = \\frac{\\tilde{v}_2}{\\|\\tilde{v}_2\\|_2}\n",
    "$$\n",
    "\n",
    "Esse processo continua, sempre removendo a contribuição dos vetores anteriormente construídos $v_j, j=1..k-1$, de $u_k$ de forma a $\\tilde{v}_k$ seja ortogonal aos $v_j$ anteriores, e por fim normalizamos $\\tilde{v}_k$.\n",
    "\n",
    "$$ \\tilde{v}_k = u_k - \\alpha_{k1} v_1 - \\alpha_{k2} v_2 -  ... - \\alpha_{k k-1} v_{k-1} \\\\ \n",
    "<\\tilde{v}_k, v_j> = 0, \\forall j=1..k-1 \\\\\n",
    "\\alpha_{kj} = <u_k, v_j> \\\\\n",
    "v_k = \\frac{\\tilde{v}_k}{\\|\\tilde{v}_k\\|_2}\n",
    "$$\n",
    "\n",
    "Esse processo de ortonormalização pode ser utilizado para calcular da decomposição QR, já que por construção  os vetores calculados são unitários e assim temos a matriz Q, e a estrutura dos coeficientes $\\alpha_{ij}$ seguem uma forma triangular e portanto podemos construção a matriz R a partir deles.\n",
    "\n",
    "Podemos entender os vetores $v_k$ como uma base, e os coeficientes $\\alpha_{ij}$ como os coeficientes nessa base que representam os vetores originais $u_j$. Nessa perspectiva podemos encarar os vetores $u_j$ como as colunas da matriz $A$.\n",
    "\n",
    "$$\n",
    "u_k = \\tilde{v}_k + \\alpha_{k1} v_1 + \\alpha_{k2} v_2 + ... + \\alpha_{kk-1} v_{k-1} \\\\\n",
    "\\tilde{v}_k = v_k \\| \\tilde{v}_k \\|_2 \\\\\n",
    "u_k = \\alpha_{k1} v_1 + \\alpha_{k2} v_2 + ... + \\alpha_{kk-1} v_{k-1} + \\alpha_{kk} v_k \\\\\n",
    "\\alpha_{kk} = \\| \\tilde{v}_k \\|_2 \\\\\n",
    "$$\n",
    "\n",
    "Dessa forma fica mais fácil ver a multiplicação das matrizes Q e R, onde Q tem os vetores $v_k$ e R tem os coeficientes $\\alpha_{ij}$.\n",
    "\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a_{1} & a_{2} & a_{3} & \\dots  & a_{n} \\end{bmatrix}, a_i \\in \\mathbb{R}^m\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " a_1 & a_2 & a_3 & \\dots  & a_n \\end{bmatrix}_{m \\times n}  =\n",
    "\\begin{bmatrix}\n",
    " q_1 & q_2 & q_3 & \\dots  & q_n \\end{bmatrix}_{m \\times n} \n",
    "\\left[ \\begin{array}{cccc|c}\n",
    "    \\alpha_{11} & \\alpha_{21} & \\dots & \\alpha_{n1} \\\\\n",
    "    0 & \\alpha_{22} & \\dots & \\alpha_{n2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & \\alpha_{nn}\n",
    "    \\end{array}\n",
    "    \\right]_{n \\times n} \\\\\n",
    "q_i \\in \\mathbb{R}^m, \\alpha_{ii} = \\| \\tilde{v}_i \\|_2 \\\\\n",
    "$$\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "a_1 = \\alpha_{11}q_1 \\\\\n",
    "a_2 = \\alpha_{21}q_1 + \\alpha_{22}q_2 \\\\\n",
    "a_3 = \\alpha_{31}q_1 + \\alpha_{32}q_2 + \\alpha{33}q_3 \\\\\n",
    "\\vdots \\\\\n",
    "a_n = \\alpha_{n1}q_1 + \\alpha_{n2}q_2 + \\alpha_{n3}q_3 + \\dots + \\alpha_{nn}q_n\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Apesar de podermos utilizar Gram-Schmidt para obter a decomposição QR, na prática ela não é utilizada devido à instabilidades. Depois falaremos sobre isso, mas adiantando um pouco, Gram-Schmidt tende e alinhar os vetores, e no fim do processo eles não formam mais um conjunto LI.\n",
    "\n",
    "A vantagem dessa decomposição é que tendo $A=QR \\implies Ax=b \\implies QRx=b \\implies Rx=Q^tb$, pois $Q$ é unitária, logo sua inversa é sua transposta (no caso real, para o caso complexo é preciso conjugá-la também). E como R é triangular o sistem $Rx=c$ é fácil de resolver.\n",
    "\n",
    "Em geral, procuramos matrizes unitárias para manipular sistemas numéricos pois elas são mais estáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Autovalores e autovetores\n",
    "Mudando para autovalores e autovetores.\n",
    "\n",
    "Dada uma matriz $A_{m \\times n}$, um vetor $v \\neq 0$ é um autovetor de $A$ se existe um número $\\lambda$ tal que $Av=\\lambda v$. Passando $\\lambda v$ para o outro lado temos: $(A-\\lambda I)_{m \\times n}v=0$. Ou seja, o vetor $v$ é autovetor se $Bv=0 \\implies det(B)=0$, pois $v \\neq 0$ que seria a solução trivial. Outra forma de pensar, é que a matriz $(A-\\lambda I)$ não tem um núcleo trivial ($\\{0\\}$, apenas o vetor nulo).\n",
    "\n",
    "Lembrando que $\\displaystyle(B) = \\sum_{\\pi \\in P} Sign(\\pi) b_{1\\,\\pi_1} b_{2\\,\\pi_2} \\dots b_{n\\,\\pi_n}$, onde $P$ é o conjunto de todas as permutações de $\\{1,2, \\dots, n\\}$, $\\pi$ é uma dada permutação, e $\\pi_i$ é a i-ésima componente da permutação $\\pi$.\n",
    "\n",
    "$b_{1\\,\\pi_1} b_{2\\,\\pi_2} \\dots b_{n\\,\\pi_n}$ é um polinômio de grau no máximo n. Sendo a variável desse polinômio o $\\lambda$, e de grau no máximo n pois a combinação $b_{1\\,\\pi_1} b_{2\\,\\pi_2} \\dots b_{n\\,\\pi_n}$ pode ser uma multiplicação de n $\\lambda s$. Logo, $det(B)$ é um polinômio de grau no máximo n.\n",
    "\n",
    "Outra expressão para esse determinante:\n",
    "\n",
    "$det(A - \\lambda I) = (-1)^n (\\lambda^n - P_1\\lambda^{n-1} + P_2\\lambda^{n-2} \\dots P_n) = P_n(\\lambda)$, onde o primeiro $P_n$ é o n-ésimo coeficiente do polinômio, é um escalar obtido do cálculo do determinante (o termo independente do polinômio característico). Já o termo $P_n(\\lambda)$ é o polinômio característico em si, por isso tem em sua expressão a variável independente $\\lambda$.\n",
    "\n",
    "Fazendo a expansão de $det(A - \\lambda I)$, é possível ver que o $P_1$ equivale ao traço da matriz, e $(-1)^{n+1}P_n = det(A)$. O traço de uma matriz é a soma dos termos da diagonal ($Tr(A)$).\n",
    "\n",
    "Os autovalores da matriz $A$ são as raízes do polinômio característico $P_n(\\lambda)$ associado à matriz $A$. Toda matriz tem $n$ autovalores se admitirmos raízes complexas.\n",
    "\n",
    "Cada autovalor não necessariamente tem um autovetor associado, ex:\n",
    "\n",
    "$A = \\displaystyle \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\implies \\lambda = 0$, esse autovalor aparece como raíz de $P_n(\\lambda)$ duas vezes, então ele tem multiplicidade 2, e tem apenas um autovetor associado, $v=e_1$ ($e_i$ é um elemento da base canonica $\\left[ \\begin{array}{}0 & 0 & \\dots & 1 & 0 & 0 \\end{array}\\right]$)\n",
    "\n",
    "A multiplicidade algébrica (A.M.) de um autovalor é o número de vezes que ele é uma raíz de $P_n(\\lambda$), enquanto que a multiplicidade geométrica (G.M) é o número de vetores LI associados à $\\lambda$. A seguinte relação é sempre respeitada $G.M. \\le A.M.$, se a igualdade ocorre para todos os autovalores, a matriz $A$ é dita diagonalizável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema:** Autovetores associados a autovalores distintos são LI\n",
    "A prova é deixada como exercício\n",
    "\n",
    "---\n",
    "Resolução:\n",
    "Supomos que $\\lambda_1, \\lambda_2$ são autovalores distintos de $A$, e seus autovetores são respectivamente $v_1,v_2$.\n",
    "Portanto temos as seguintes relações:\n",
    "$$\n",
    "Av_1=\\lambda_1v_1 \\\\\n",
    "Av_2=\\lambda_2v_2 \\\\\n",
    "\\text{Supondo que } v_1,v_2 \\text{ não são LI, temos } v_1 = \\alpha v_2, \\text{ logo, subtraindo as duas equações acima temos } \\\\\n",
    "Av_1 - Av_2 = \\lambda_1v_1 - \\lambda_2v_2 \\implies A(v_1 - v_2) = \\lambda_1v_1 - \\lambda_2v_2 \\\\\n",
    "A(\\alpha v_2 - v_2) = \\lambda_1 \\alpha v_2 - \\lambda_2 v_2 \\\\\n",
    "A(\\alpha - 1)v_2 = (\\lambda_1 \\alpha - \\lambda_2)v_2 \\\\\n",
    "A v_2 = \\frac{\\lambda_1 \\alpha - \\lambda_2}{\\alpha - 1} v_2\n",
    "$$\n",
    "Chegamos num absurdo, pois supusemos que o autovalor associado ao autovetor $v_2$ era $\\lambda_2$ e não $\\displaystyle \\frac{\\lambda_1 \\alpha - \\lambda_2}{\\alpha - 1}$.\n",
    "\n",
    "Com isso concluímos que $v_1$ e $v_2$ são LI, e como eles foram escolhidos arbitrariamente, quaisquer dois autovetores associados a autovalores distinto são LI.\n",
    "\n",
    "---\n",
    "\n",
    "Adiantando um pouco o que veremos no final, o Power method, ou método das potências, é utilizado para encontrar os autovalores e autovetores, se o autovetor é conhecido, é trivial descobrir seu autovalor associado. É preciso apenas utilizar o Rayleigh quotient, $\\lambda = \\displaystyle \\frac{x^t x}{x^tAx}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação de similaridade\n",
    "Uma matriz $A$ é dita ser similar a $B$ se existe uma matriz $S$ invertível tal que $A = S^{-1} B S$.\n",
    "\n",
    "\n",
    "### Matriz diagonalizável\n",
    "Uma matriz é diagonalizável se e somente se existe uma matriz $S$ invertível tal que $S^{-1} A S = D$, onde $D$ é uma matriz diagonal\n",
    "\n",
    "Supondo que uma matriz $A$ tenha um conjunto completo de autovetores (do inglês, *complet set of eigenvectors*), ou seja, $A_{n \\times n}$ tem n autovetores, então $A$ é diagonalizável.\n",
    "\n",
    "Agrupando os autovetores de $A$ como colunas de uma matriz $S$, $S = \\left[ \\begin{array}{} v_1 & v_2 & \\dots & v_n \\end{array}\\right]$, onde $v_i$ é o i-ésimo autovetor de $A$, e os autovalores na diagonal de uma matriz $D$, $D = \\displaystyle \\left[\n",
    "\\begin{array}{}\n",
    "\\lambda_1 & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_2 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_n\n",
    "\\end{array}\\right]$, obtemos a seguinte relação:\n",
    "\n",
    "$$\n",
    "A v_1 = \\lambda_1 v_1 \\\\\n",
    "S^{-1} A S = D \\\\\n",
    "A S = S D \\\\\n",
    "AS = \\left[ \\begin{array}{} Av_1 & Av_2 & \\dots & Av_n \\end{array} \\right] =\n",
    "\\left[ \\begin{array}{} \\lambda_1 v_1 & \\lambda_2 v_2 & \\dots \\lambda_n v_n \\end{array} \\right] =\n",
    "\\left[ \\begin{array}{} v_1 & v_2 & \\dots v_n \\end{array} \\right] \n",
    "\\left[\n",
    "\\begin{array}{}\n",
    "\\lambda_1 & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_2 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_n\n",
    "\\end{array}\\right] = SD\n",
    "$$\n",
    "\n",
    "**exercício:** Mostrar $A = \\displaystyle \\left[\n",
    "\\begin{array}{}\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]$ que não é diagonalizável.\n",
    "\n",
    "---\n",
    "Resolução:\n",
    "\n",
    "Tomando uma matriz genérica $S = \\displaystyle \\left[ \\begin{array}{} a & b \\\\ c & d \\end{array}\\right]$, podemos escrever a inversa $S^{-1} = \\displaystyle \\left[ \\begin{array}{} d & -b \\\\ -c & a \\end{array}\\right] \\times \\frac{1}{ad-bc} $, o que implica $S^{-1} A S = \\displaystyle \\left[ \\begin{array}{} d_1 & 0 \\\\ 0 & d_2 \\end{array}\\right]$.\n",
    "\n",
    "Fazendo a maltiplicação das matrizes obtemos a matriz $\\displaystyle \\left[ \\begin{array}{} cd & -c^2 \\\\ d^2 & -cd \\end{array}\\right]$, e essa matriz deve ser igual à diagonal, logo $\\displaystyle \\left[ \\begin{array}{} cd & -c^2 \\\\ d^2 & -cd \\end{array}\\right] = \\displaystyle \\left[ \\begin{array}{} d_1 & 0 \\\\ 0 & d_2 \\end{array}\\right]$. Podemos concluir que $c=d=0$. \n",
    "\n",
    "Tendo $c=d=0$ o inverso do determinante de $S$, usado no cálculo da inversa de $S$, não é definido $\\Big( \\displaystyle \\frac{1}{0} \\Big)$, logo não é possível calcular a inversa de $S$.\n",
    "Portanto, não é possível construir uma matriz que torne $A$ similar a uma diagonal.\n",
    "\n",
    "Outra maneira de responder essa questão seria verificar que o autovalor $\\lambda_1=0$ tem multiplicidade 2, e apenas um autovetor associado, logo $A$ não tem um conjunto completo de autovetores e portanto não é diagonalizável. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Algumas propriedades:\n",
    "- Toda matriz simétrica é diagonalizável\n",
    "- Toda matriz normal é digonalizável. Matriz normal é a generalização de matriz simétrica, ela respeita a seguinte expressão $A^H A = A A^H$. Esta é a classe mais geral de matrizes que sabemos ser diagonalizável\n",
    "- A propriedade de uma matriz ser diagonalizável não é robusta à perturbações. Isso significa que pequenas perturbações numa matriz diagonalizável pode torná-la não diagonalizável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forma canônica de Jordan, ou forma normal de Jordan\n",
    "\n",
    "Antes de definir a forma canônica de Jordan definimos o bloco de Jordan associado ao autovalor $\\lambda_i$\n",
    "\n",
    "$$\n",
    "J_m(\\lambda_i) = \\left[\n",
    "\\begin{array}{}\n",
    "\\lambda_i & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_i & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_i & 1 \\\\\n",
    "0 & 0 & \\dots & 0 & \\lambda_i\n",
    "\\end{array}\\right]_{m \\times m}\n",
    "$$\n",
    "\n",
    "Os blocos de Jordan associados aos autovalores que não possuem uma quantidade de autovetores igual a sua multiplicidade apresentam essa subdiagonal superior de 1's. Caso os autovalores sejam simples, os blocos de Jordan serão matrizes $1 \\times 1$\n",
    "\n",
    "**Teorema de Jordan:** Seja $A_{n \\times n}$, uma matriz quadrada, então existe uma matriz $X$ invertível tal que $X^{-1} A X = \\left[\n",
    "\\begin{array}{cccc}\n",
    "J_{n1}(\\lambda_1) & 0 & \\dots & 0 \\\\\n",
    "0 & J_{n2}(\\lambda_2) & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & J_{nk}(\\lambda_k)\n",
    "\\end{array}\\right]_{n \\times n}$, onde $n=n_1 + n_2 + \\dots + n_k$\n",
    "\n",
    "Cada $n_i = max\\{l \\text{ tal que } ker(A - \\lambda_i I)^l \\subset ker(A- \\lambda_i I)^{l+1}\\}$, é a dimensão do maior autoespaço associado a esse autovalor, isso leva em conta o problema dos autovalores generalizados. Um autovalor pode ter um multiplicidade algébrica 3 e multiplicidade geométrica 1, então é possível encontrar um autovetor associado a esse autovalor, mas os outros 2 autovetores não são encontrados. Nesse caso é preciso encontrar os autovetores generalizados.\n",
    "\n",
    "\n",
    "Uma utilidade da forma de Jordan é para resolução de equações diferenciais (EDO), $\\dot{x}=Ax$. Usando a forma de Jordan, é possível definir exponenciais de matriz como resposta desse sistema de equações, $\\displaystyle x(t) = e^{At}$, decompondo $A$, é possível definir $\\displaystyle e^{J_n}$ como uma séria de potência e fica fácil analisar a convergência. A subdiagonal de 1's relacionada aos autovetores generalizados tende a sumir, conforme $\\displaystyle e^{J_n}$ é iterada essa subdiagonal é deslacada para o direita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
