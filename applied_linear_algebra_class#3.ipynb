{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 03\n",
    "\n",
    "Recapitulando a forma canonica de Jordan\n",
    "\n",
    "### Forma canônica de Jordan, ou forma normal de Jordan\n",
    "\n",
    "Dada uma matriz $A$, por transformação de similaridade encontramos a seguinte matriz:\n",
    "\n",
    "$$\n",
    "S^{-1} A S = \n",
    "\\begin{bmatrix}\n",
    "J_{n1}(\\lambda_1) & 0 & \\dots & 0 \\\\\n",
    "0 & J_{n2}(\\lambda_2) & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & J_{nk}(\\lambda_k)\n",
    "\\end{bmatrix}\n",
    "\\;\n",
    "J_i(\\lambda_i) = \n",
    "\\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_i & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_i & 1 \\\\\n",
    "0 & 0 & \\dots & 0 & \\lambda_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Um problema com a forma de Jordan é que a função que pega qualquer matriz e leva em sua forma canonica de Jordan é descontínua. Descontínua no seguinte sentido, uma pequena perturbação na matriz $A$ pode mudar totalmente a forma de Jordan.\n",
    "\n",
    "Uma maneira de exergar a descontinuidade é utilizando sequências. Segundo o professor, no livro do Golub ele mostra isso, quando estava estudando não havia encontrado, mas encontrei um exemplo em que a descontinuidade ocorre e utilizei isso para resolver um exercicio.\n",
    "\n",
    "---\n",
    "**Exercício:** Mostrar que a função que leva uma matriz em sua forma de Jordan é descontínua.\n",
    "\n",
    "O que fiz foi achar um exemplo em que a descontinuidade ocorre.\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\mathbb{R}^{m \\times n} \\; \\to \\; \\mathbb{R}^{m \\times n}\\\\\n",
    "A \\;\\;\\; \\to  \\;\\;\\; J\n",
    "\\end{array},\n",
    "F(A) = J\n",
    "$$\n",
    "\n",
    "Uma maneira de mostra a descontinuidade por meio de sequências é da seguinte forma:\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "x_n \\to x\\\\\n",
    "F(x_n) \\not\\to F(x)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Isso significa que uma sequência $x_n$ tende a $x$, e uma função $F$ se ela aplicada ao elemento genérico da sequência $x_n$ não convergir para a função aplicada o ponto de acumulação da sequência $x$ significa que a função não é contínua.\n",
    "\n",
    "E essa será a forma para mostrar um caso em que a função que leva uma matriz a sua forma de Jordan não é contínua.\n",
    "\n",
    "Tomando a sequência de matrizes $A_n =\\displaystyle \\begin{bmatrix}\n",
    "\\frac{1}{n} & 1 \\\\\n",
    "0 & -\\frac{1}{n}\n",
    "\\end{bmatrix}$, temos que ela tende à matriz $A =\\displaystyle \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Enquanto que a forma de Jordan de $A_n$ tende à $\\displaystyle \\begin{bmatrix}\n",
    "\\frac{1}{n} & 0 \\\\\n",
    "0 & -\\frac{1}{n}\n",
    "\\end{bmatrix}$, a forma de Jordan de $A$ é $\\displaystyle \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "Já havíamos visto essa matriz e como ela é defectiva, ou seja, não possui um conjunto completo de autovetores, isso se reflete em sua forma de Jordan. A matriz $A$ possui autovalor zero com multiplicidade 2, seu bloco de jordan reflete o 1 na subdiagonal.\n",
    "\n",
    "A matriz $A_n$ possui autovalores $\\displaystyle \\frac{1}{n}$ e $\\displaystyle -\\frac{1}{n}$, logo possui um conjunto completo de autovetores e sua forma de Jordan é diagonal.\n",
    "\n",
    "Como as entradas de $F(A_n)$ não tendem às entradas de $F(A)$, a função $F$ não é contínua.\n",
    "\n",
    "Este fato está relacionado à multiplicidade dos autovalores e como uma perturbação na matriz afeta seu bloco de Jordan correspondente.\n",
    "\n",
    "Uma analogia que o professor fez foi quanto a função que encontra as raízes de um polinômio, no corpo dos números reais, ela também é descontínua porque pequenas perturbações pode mudar radicalmente as raízes. Pense na situação de uma parábola que só tem uma raíz, qualquer perturbação que descole a parábola do eixo x faz com que não exista mais raízes, daí a descontinuidade.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decomposição de Schur\n",
    "\n",
    "**Teorema:** Toda matriz quadrada é unitariamente silimar a uma matriz triangular superior, $A = U^H T U$.\n",
    "\n",
    "Isso é bom porque os autovalores ficam na diagonal da matriz triangular. Usaremos essa decomposição quando estudarmos métodos para encontrar os autovalores e autovetores.\n",
    "\n",
    "**Prova:**\n",
    "\n",
    "A prova desse teorema é por indução na dimensão da matriz.\n",
    "\n",
    "Se $n=1$, $A=[a_{11}] \\implies 1.a_{11}.1$, onde $U=[1]$. Para $n=1$ o teorema é trivial.\n",
    " \n",
    "Suponha que o teorema é válido para toda matriz $(n-1) \\times (n-1)$. Seja $A$ uma matriz $n \\times n$, e $\\lambda, u$ um par autovalor e autovetor ($Au= \\lambda u$) com $\\|u\\|_2 = 1$.\n",
    "\n",
    "Agora, é possível encontrar $\\tilde{U}_{n \\times (n-1)}$ tal que $U = [u \\; \\tilde{U}]$ seja uma matriz unitária. Isso é possível, pois pode-se completar uma base dada um conjunto inicial de vetores LI, de forma que essa base seja ortonormal.\n",
    "\n",
    "Com isso temos:\n",
    "\n",
    "$$\n",
    "U^HAU = \\begin{bmatrix} u^H \\\\ \\tilde{U}^H\\end{bmatrix} A \\begin{bmatrix} u & \\tilde{U}\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "(u^H A u)_{1 \\times 1}  & (u^H A \\tilde{U})_{1 \\times n-1} \\\\\n",
    "(\\tilde{U}^H A u)_{1 \\times 1} & (\\tilde{U}^H A \\tilde{U})_{n-1 \\times n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} \\lambda & \\mathbf{a'_{12}} \\\\ 0 & \\tilde{A}_{n-1 \\times n-1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Pois, como a norma do autovetor é 1, o produto $u^H A u = u^H (\\lambda u) = \\lambda u^H u = \\lambda$. Já o vetor $\\mathbf{a'_{12}}$ é fruto da multiplicação $u^H A \\tilde{U}$. O 0 é obtido pois, $\\tilde{U}^H A u = \\tilde{U}^H (\\lambda u) = \\lambda (\\tilde{U}^H u) = 0$, já que $\\tilde{U}$ e $u$ são ortogonais por construção de $\\tilde{U}$.\n",
    "\n",
    "Como supomos que as hipóteses do teorema são válidas para $n-1$, é possível decompor $\\tilde{A}$.\n",
    "\n",
    "Ou seja, $\\exists \\; P$ tal que $\\tilde{A} = P \\tilde{T} P^H$. Utilizando essa decomposição na matriz anterior teremos:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\lambda & \\mathbf{a'_{12}} \\\\ 0 & \\tilde{A}\\end{bmatrix} =\n",
    "\\begin{bmatrix} \\lambda & \\mathbf{a'_{12}} \\\\ 0 & P \\tilde{T} P^H \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 1 & 0 \\\\ 0 & P\\end{bmatrix} \\begin{bmatrix} \\lambda & \\mathbf{a'_{12}} \\\\ 0 & \\tilde{T}\\end{bmatrix}\n",
    "\\begin{bmatrix} 1 & 0 \\\\ 0 & P^H\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Onde $H = \\displaystyle \\begin{bmatrix} 1 & 0 \\\\ 0 & P\\end{bmatrix}$, e $H H^H = \\displaystyle \\begin{bmatrix} 1 & 0 \\\\ 0 & PP^H\\end{bmatrix}= I_{n \\times n}$,ou seja, $H$ é unitária.\n",
    "\n",
    "Ou seja, $H^H U^H A U H = \\displaystyle H \\begin{bmatrix} \\lambda & \\mathbf{x} \\\\ 0 & \\tilde{T}\\end{bmatrix} H^H$ que é triangular superior como queríamos mostrar. E como o produto de unitárias é unitária ($(UH)(UH)^H = UHH^HU^H = U I U^H = I $), a matriz $A$ é unitariamente similar à uma matriz triangular superior, como queríamos demonstrar.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exercício:** Mostrar que toda matriz normal é diagonalizável.\n",
    "\n",
    "Se $A$ é normal, então $A^H A = A A^H$, $A$ e $A^H$ têm os mesmos autovalores, e como $A^H A$ é simétrica seus autovalores são reais.\n",
    "\n",
    "$A^H A = S^H T S$ pela decomposição de Schur, com $T$ triangular superior.\n",
    "\n",
    "Mas $(A^H A)^H = (S^H T S)^H = S^H T^H S$, logo $T$ tem que ser diagonal com os autovalores na diagonal, esses autovalores são os quadrados dos autovalores de $A$. E como utilizamos a decomposição de Schur, a matriz $S$ é unitária, a decomposição de Schur para $A^H A$ é equivalente à decomposição por autovalores e autovetores. \n",
    "\n",
    "Podemos representar essa decomposição da seguinte forma $A^H A = S^H T S = S^H B B S = S^H B S S^H B S = (S^H B S) (S^H B S) = A^H A$, onde $B = T^{-\\frac{1}{2}}$ e $S$ é composta pelos autovetores normalizados de $A$ e $S^H$ os autovetores normalizados de $A^H$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposição por valores singulares (SVD)\n",
    "\n",
    "**Teorema:** Seja $A_{m \\times n}$ uma matriz qualquer, então $\\exists \\; U_{m \\times n}, V_{n \\times n}$ unitárias tal que $U^H_{m \\times m} A_{m \\times n} V_{n \\times n} = \\Sigma_{m \\times n}$, com:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_1 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & \\sigma_2 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & \\ddots & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\sigma_p & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}, \\sigma_1 \\geq \\sigma_2 \\geq \\dots \\sigma_p \\geq 0, \\sigma_i = \\text{i-ésimo valor singular de } A\n",
    "$$\n",
    "\n",
    "---\n",
    "A decomposição por valores singulares (SVD) não é uma transformação de similaridade, mas tem relação com os autovalores de $A$.\n",
    "Na verdade a relação é essa $\\sigma_i = \\sqrt{\\lambda_i}$\n",
    "\n",
    "\n",
    "O professor indicou que no livro do Golub tem a prova deste teorema, e se não me engano também é por indução, depois procuro ela e coloco aqui.\n",
    "\n",
    "\n",
    "Podemos calcular a decomposição SVD das seguintes matrizes:\n",
    "- $A^H_{n \\times m} A_{m \\times n} = B_{n \\times n}$, $B$ possui n autovalores e autovetores, como é simétrica os autovalores são reais. Para o caso de $A$ no corpo dos reais, os autovalores também serão positivos;\n",
    "- $A_{m \\times n} A^H_{n \\times m} = C_{m \\times m}$, $C$ possui m autovalores e autovetores, como é simétrica os autovalores são reais. Para o caso de $A$ no corpo dos reais, os autovalores também serão positivos.\n",
    "\n",
    "$B$ e $C$ têm os mesmos autovalores não zeros.\n",
    "\n",
    "$V = [v_1  v_2  \\dots  v_m]$ são os autovetores de B\n",
    "\n",
    "$U = [u_1  u_2  \\dots  u_m]$ são os autovetores de C\n",
    "\n",
    "$U$ e $V$ são as matrizes da decomposição SVD de $A$ ($A = U \\Sigma V^H$)\n",
    "\n",
    "$A^H A = V \\Sigma^H U^H U \\Sigma V^H = V \\Sigma^H \\Sigma V^H$. Neste caso temos uma transformação de similaridade pois $U$ e $V$ são matrizes unitárias.\n",
    "\n",
    "$$\n",
    "\\Sigma^H \\Sigma = \\begin{bmatrix}\n",
    "|\\sigma_1|^2 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & |\\sigma_2|^2 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & \\ddots & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & |\\sigma_p|^2 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}, |\\sigma_1|^2, |\\sigma_2|^2, \\dots |\\sigma_p|^2 \\text{são os autovalores de } A^H A\n",
    "$$\n",
    "\n",
    "$A A^H = U \\Sigma V^H V \\Sigma^H U^H = U \\Sigma \\Sigma^H U^H$, este caso também também é uma transformação de similaridade.\n",
    "\n",
    "$$\n",
    "\\Sigma^H \\Sigma = \\begin{bmatrix}\n",
    "|\\sigma_1|^2 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & |\\sigma_2|^2 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & \\ddots & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & |\\sigma_q|^2 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}, |\\sigma_1|^2, |\\sigma_2|^2, \\dots |\\sigma_q|^2 \\text{são os autovalores de } A A^H\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pseudo-inversa de Moore-Penrose (Inversa generalizada)\n",
    "\n",
    "Tipicamente usa-se o símbolo $^\\dagger$ para designar a pseudo-inversa.\n",
    "\n",
    "$A^\\dagger_{m \\times n} = V_{n \\times n} \\Sigma^\\dagger_{n \\times m} U^H_{m \\times m}, \n",
    "\\; \\displaystyle \\Sigma^\\dagger = \\begin{bmatrix}\n",
    "\\frac{1}{\\sigma_1} & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & \\frac{1}{\\sigma_2} & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & \\ddots & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\frac{1}{\\sigma_p} & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$A A^\\dagger = U \\Sigma V^H V \\Sigma^\\dagger U^H = U \\Sigma \\Sigma^\\dagger U^H = I$, pois $\\Sigma \\Sigma^\\dagger = I$\n",
    "\n",
    "A pseudo-inversa é a solução do problema de minimização $min_x \\| Ax - b \\|_2$, $x = A^\\dagger b$. O que significa a projeção do vetor $b$ no subespaço definido pelo matriz $A$. Pode ser que a matriz $A$ é defectiva e o vetor $b$ não está no espaço coluna de $A$, essa minimização tenta encontrar o vetor $x$ no espaço coluna de $A$ que mais se aproxima de $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise de erro\n",
    "\n",
    "Começaremos agora a parte da análise de erros. Para isso introduziremos conceitos de produto interno e normas.\n",
    "\n",
    "#### Produto interno\n",
    "\n",
    "O produto interno é uma função que toma dois argumentos e leva num número real, há várias notações para o produto interno, aqui usaremos $<.,.>$, onde os pontos denotam os argumentos. \n",
    "\n",
    "$$ < \\cdot, \\cdot > : V \\times V \\to \\mathbb{C} \\; ou \\; \\mathbb{R} $$\n",
    "\n",
    "Para ser produto interno é preciso respeitar as seguintes propriedades:\n",
    "- $<x,y> = \\overline{<y,x>}$ (simétrico conjugado);\n",
    "- $<x + \\alpha y, z> = <x,z> + \\alpha<y,z>$ (linearidade);\n",
    "- $<x,x> \\geq 0 \\; \\forall x \\in V, x \\neq 0$ (positividade para elemento não nulo).\n",
    "\n",
    "### Norma\n",
    "\n",
    "De forma similar, a norma é uma função que leva um elemento do conjunto $V$ ao reais positivos, ou seja:\n",
    "$$ \\| \\cdot \\| : V \\to \\mathbb{R}^+ $$\n",
    "\n",
    "A norma também precisa satisfazer algumas propriedades:\n",
    "- $\\|x\\| \\geq 0, \\; \\|x\\|=0 \\iff x = 0$ (positividade para elemento não nulo);\n",
    "- $\\| \\alpha x \\| = |\\alpha| \\|x\\|$ ();\n",
    "- $\\|x+y \\| \\leq \\|x\\| + \\|y\\|$ (desigualdade triangular).\n",
    "\n",
    "Uma norma pode ser induzida de um produto interno, mas não entraremos nesse nível de detalhe agora.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Norma de vetores\n",
    "\n",
    "Vamos começar falando de norma de vetores. Uma p-norma é definida da seguinte forma: \n",
    "$$\n",
    "\\|x\\|_p = (\\sum_{i=1}^n |x_i|^p)^\\frac{1}{p}, 1 \\leq p \\leq \\infty \\to \\|x\\|_1, \\|x\\|_2, \\dots \\|x\\|_\\infty\n",
    "$$\n",
    "$x_i$ é a i-ésima componente do vetor $x$.\n",
    "\n",
    "Como podemos identificar uma matriz $A_{m \\times n}$ com um vetor de dimensão $\\mathbb{R}_{m \\times n}$. Aqui o verbo identificar significa que podemos considerar a matriz como um vetor, pode-se pensar como uma serialização da matriz, ou pegar a matriz e esticar linhas até virar um vetor.\n",
    "\n",
    "Para os que não viram esse verbo usado nesse contexto, signifca que você consegue fazer uma ligação entre os elementos dos dois objetos, por exemplo, você pode fazer a primeira linha da matriz ser atribuída ao primeiros n elementos do vetor, depois você atribui os próximos n elementos do vetor com a segunda linha da matriz, e assim por diante.\n",
    "\n",
    "Logo, podemos utilizar as normas de vetores para matrizes.\n",
    "\n",
    "**Definição:** Duas normas são ditas equivalentes se $\\exists \\; c_1, c_2 \\in \\mathbb{R}$ tal que $c_1 \\|x\\|_p \\leq \\|x\\|_q \\leq c_2\\|x\\|_p$. Ou seja, é possível limitar uma norma $\\| \\cdot \\|_q$ por outra $\\| \\cdot \\|_p$ por cima e por baixo a menos de constantes $c_1$ e $c_2$.\n",
    "\n",
    "Isso é importante pois retira a importância da escolha da norma para as provas de convergência. Você pode trabalhar com a norma que parecer mais fácil para o problema em questão e há  a garantia que a mesma prova será válida para outra norma equivalente.\n",
    "\n",
    "$$\n",
    "x_n \\to y \\implies \\| x_n - y \\| \\to 0\\\\\n",
    "\\| x_n -y \\|_p < \\epsilon, \\forall \\; n \\; \\text{large}\\\\\n",
    "\\| x_n -y \\|_q < \\epsilon, \\forall \\; m \\geq n \\; \\text{large}\\\\\n",
    "$$\n",
    "\n",
    "Outro fato importante é que em espaços de dimensão finita, como $\\mathbb{R}^n$ ou $\\mathbb{R}^{m \\times n}$, todas as normas são equivalentes. Esse fato é tão importante que é até um teorema.\n",
    "\n",
    "**Teorema:** Em espaços de dimensão finita, todas as normas são equivalentes.\n",
    "\n",
    "A prova do teorema não foi dada em aula, não tenho certeza, mas acho que no livro do Halmos tem a prova, depois eu checo isso e se tiver eu incluo. Não achei no livro do Halmos, mas no seguinte link ( http://mathonline.wikidot.com/equivalence-of-norms-in-a-finite-dimensional-linear-space ) há essa prova, depois eu transcrevo ela aqui.\n",
    "\n",
    "---\n",
    "\n",
    "Podemos definir a 1-norma da seguinte forma: $\\displaystyle \\|A\\|_1 = \\sum_{i=1}^m\\sum_{j=1}^n |a_{ij}|$.\n",
    "\n",
    "O problema com norma de vetores para matrizes é que elas não são muito úteis por não têm a seguinte propriedade:\n",
    "$$ \\|Ax\\| \\leq \\|A\\| \\|x\\|$$\n",
    "\n",
    "Essa propriedade, segundo o livro do Quarteroni, significa que a norma de matriz é compatível, ou consistente, com a norma de vetor.\n",
    "\n",
    "**Definição:** Dizemos que uma norma de matriz é submultiplicativa se $\\forall \\; A \\in \\mathbb{R}^{n \\times m}, \\forall \\; B \\in \\mathbb{R}^{m \\times q}, \\|AB\\| \\leq \\|A\\| \\|B\\|$.\n",
    "\n",
    "As normas de vetores aplicadas a matrizes não satisfazem essa propriedade, mas a seguinte norma sim.\n",
    "\n",
    "**Norma de Frobenius** é definida da seguinte forma: $\\displaystyle \\|A\\|_F = \\sqrt{\\sum_{i,j=1}^n |a_{ij}|^2} = \\sqrt{Tr(AA^H)}$\n",
    "\n",
    "Essa norma é submultiplicativa e é compatível com a 2-norma (euclidiana)\n",
    "\n",
    "$$ \\|Ax\\|^2_2 = \n",
    "\\sum_{i=1}^n\\Bigg| \\sum_{j=1}^n a_{ij} x_j\\Bigg|^2 \\leq \n",
    "\\sum_{i=1}^n\\Bigg( \\sum_{j=1}^n |a_{ij}|^2 \\sum_{j=1}^n |x_j|^2\\Bigg) = \n",
    "\\|A\\|^2_F \\|x\\|^2_2$$\n",
    "\n",
    "Nessa norma temos que $\\|I_n\\|_F = \\sqrt{n}$\n",
    "\n",
    "Portanto, a seguir definiremos normas para matrizes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Norma de matriz\n",
    "\n",
    "Norma de matriz é definida da sequinte forma:\n",
    "$$ \\|A\\| = \\sup_{\\|x\\| \\neq 0} \\frac{\\|Ax\\|_p}{\\|x\\|_q} = \\sup_{\\|y\\| = 1} \\|Ay\\| $$\n",
    "\n",
    "Essa é chamada de **norma induzida** de matriz ou norma natural de matriz. Atenção que os índices das normas do primeiro supremo não necessariamente são iguais.\n",
    "\n",
    "Pela propriedade do supremo, temos que $\\displaystyle \\|A\\| \\geq \\frac{\\|Ax\\|}{\\|x\\|} \\implies \\|Ax\\| \\leq \\|A\\|\\|x\\|$\n",
    "\n",
    "Como estamos trabalhamos num espaço de dimensão finita e normalmente em conjuntos compactos, o supremo é o mesmo que o máximo, ou seja:\n",
    "$$ \\|A\\| = \\max_{1 \\leq \\; i \\,ou\\, j\\; \\leq n \\\\ \\|x\\| = 1} \\sum_{j \\,ou\\, i =1}^n |a_{ij}x_j|$$\n",
    "\n",
    "Mas essa definição as vezes não é muito útil pois é de difícil cômputo, ex:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ -1 & 2 \\end{bmatrix} \\to Ax = \\begin{bmatrix} x_1 + 2x_2 \\\\ -x_1+x_2 \\end{bmatrix}\n",
    "\\implies \\|Ax\\|_1 = \\max_{\\|x\\|=1} |x_1+2x_2| + |-x_1+x_2|\n",
    "$$\n",
    "Mas não é fácil avaliar o máximo dessa expressão.\n",
    "\n",
    "**Teorema:**\n",
    "$$\\|A\\|_1 = \\max_{j=1,..n}\\sum_{i=1}^n |a_{ij}|$$\n",
    "$$\\|A\\|_2 = \\sqrt{\\rho(A^tA)} = \\sqrt{\\rho(AA^t)} = \\sigma_1(A)$$\n",
    "$$\\|A\\|_\\infty = \\max_{i=1,..n}\\sum_{j=1}^n |a_{ij}|$$\n",
    "\n",
    "$\\rho(A^tA)$ representa o maior autovalor de $A^tA$, e $\\sigma_1(A)$ o maior valor singular de $A$\n",
    "\n",
    "---\n",
    "\n",
    "Um exercício interessante seria mostrar que as normas 1 e $\\infty$ são dalas pelos respectivos máximos\n",
    "\n",
    "**Exercício:** Provar o teorema anterior para as normas 1 e $\\infty$\n",
    "\n",
    "Com esse teorema temos algumas fórmulas que faciliam o cálculo de algumas normas de matriz. Por exemplo:\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ -1 & 2 \\end{bmatrix} \\implies \\begin{array}{c} \\|A\\|_1 = max\\{2, 4\\} = 4 \\\\ \\|A\\|_\\infty = max\\{3, 3\\} = 3 \\end{array}\n",
    "$$\n",
    "\n",
    "---\n",
    "No meu caderno está marcado para tentar provar a equivalência das normas, mas não acho que foi o professor que disse para tentar resolver como exercício, mas sim eu quem colocou esse comentário. De qualquer forma vou colocar como um exercício.\n",
    "\n",
    "**Exercício:** Prova a equivalência entre essas normas de matriz.\n",
    "\n",
    "\n",
    "**Teorema:** Toda norma induzida é submultiplicativa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Raio espectral\n",
    "O raio espectral denotado por $\\rho(A)$ é o maior autovalor em módulo, ou seja: $\\rho(A) = max\\{|\\lambda|\\}$\n",
    "\n",
    "$\\rho(A)$ é quase uma norma, é uma pseudo-norma, pois não é positivo definido.\n",
    "$$ A = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}, A \\neq 0, \\rho(A) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "**Teorema:** Dada uma matriz $A \\in \\mathbb{C}^{m \\times n}$:\n",
    "1. $\\|A\\| \\geq \\rho(A)$, para qualquer norma induzida;\n",
    "2. $\\forall \\; \\epsilon > 0, \\exists \\;$ uma norma que depende de $\\epsilon$ tal que $\\|A\\|_{\\epsilon} \\leq \\rho(A) + \\epsilon$. Ou seja, $\\rho(A)$ é um ponto de acumulação das normas, serve como medida de convergência para saber se temos uma norma ou não. Posto de outra forma, dado $\\rho$, sempre é possível definir uma norma tão próximo de $\\rho$ quanto se queira.\n",
    "\n",
    "**Prova:**\n",
    "1. Seja $\\lambda$ o maior autovalor de $A$ e $u$ seu autovetor com norma 1.\n",
    "$$\n",
    "\\rho(A)=\\lambda \\implies Au=\\lambda u \\implies \\|Au\\|=|\\lambda|\\|u\\|=|\\lambda| = \\rho(A)\\\\\n",
    "\\rho(A) = \\|Au\\| \\leq \\|A\\|\\|u\\| = \\|A\\|,\n",
    "$$\n",
    "a desigualdade segue a submultiplicatividade e $\\|u\\|=1$.\n",
    "\n",
    "2. A ideia da parte 2 é escrever $A$ em sua forma de Jordan e depois fazer uma transformação de similaridade que definirá uma $\\epsilon$- norma.\n",
    "    Dada $J$ sua forma de Jordan, \n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "\\lambda_1 & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_1 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_p & 1 \\\\\n",
    "0 & 0 & \\dots & 0 & \\lambda_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Definiremos uma matriz diagonal $D_\\epsilon = diag\\{1, \\epsilon, \\epsilon^2, \\dots \\epsilon^{n-1} \\}$, e sua inversa $D_\\epsilon^{-1}$:\n",
    "$$\n",
    "D_\\epsilon = \\begin{bmatrix}\n",
    "1 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & \\epsilon^1 & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\epsilon^{n-2} & 0 \\\\\n",
    "0 & 0 & \\dots & 0 & \\epsilon^{n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Como  $D_\\epsilon$ é diagonal, sua inversa é só inverter os elementos na diagonal.\n",
    "\n",
    "Temos $J = S^{-1} A S$, e fazendo uma transformação de similaridade de J com $D_\\epsilon$, obtemos:\n",
    "$$\n",
    "D_\\epsilon^{-1} J D_\\epsilon = D_\\epsilon^{-1} S^{-1} A S D_\\epsilon = (S D_\\epsilon)^{-1} A (S D_\\epsilon)\n",
    "$$\n",
    "\n",
    "A matriz $(S D_\\epsilon)^{-1}$ é a que permitirá definirmos uma $\\epsilon$-norma, mas antes vamos ver como fica a matriz similar a J.\n",
    "\n",
    "Primeiro a multiplicação à direita\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & \\frac{1}{\\epsilon^1} & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\frac{1}{\\epsilon^{n-2}} & 0 \\\\\n",
    "0 & 0 & \\dots & 0 & \\frac{1}{\\epsilon^{n-1}}\n",
    "\\end{bmatrix}\n",
    "\\begin{pmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\lambda_1 & 1 & 0 & \\dots & 0 \\\\\n",
    "    0 & \\lambda_1 & 1 & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & \\lambda_p & 1 \\\\\n",
    "    0 & 0 & \\dots & 0 & \\lambda_p\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & 0 & \\dots & 0 \\\\\n",
    "    0 & \\epsilon^1 & 0 & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & \\epsilon^{n-2} & 0 \\\\\n",
    "    0 & 0 & \\dots & 0 & \\epsilon^{n-1}\n",
    "    \\end{bmatrix}\n",
    "\\end{pmatrix} = \\\\\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & \\frac{1}{\\epsilon^1} & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\frac{1}{\\epsilon^{n-2}} & 0 \\\\\n",
    "0 & 0 & \\dots & 0 & \\frac{1}{\\epsilon^{n-1}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & \\epsilon^1 & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_1 \\epsilon^1 & \\epsilon^2 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_p \\epsilon^{n-2} & \\epsilon^{n-1} \\\\\n",
    "0 & 0 & \\dots & 0 & \\lambda_p \\epsilon^{n-1}\n",
    "\\end{bmatrix} =\\\\\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & \\epsilon & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_1 & \\epsilon & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_p & \\epsilon \\\\\n",
    "0 & 0 & \\dots & 0 & \\lambda_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A única alteração foi a substituição dos 1's na subdiagonal por $\\epsilon$. \n",
    "\n",
    "Lembrando que transformações de similaridade não alteram os autovalores.\n",
    "\n",
    "Se utilizarmos a $\\infty$-norma nessa matriz resultante teremos que a norma de é fácil ver que $\\|\\cdot\\|_epsilon = \\rho(A) + \\epsilon$. Mostraremos como essa norma foi construída.\n",
    "\n",
    "Fazendo $\\|x\\|_\\epsilon = \\| (S D_\\epsilon)^{-1} x\\|_\\infty$, aqui a definição é para norma de vetor, podemos definir a norma de matriz pela forma da norma induzida:\n",
    "\n",
    "$$\n",
    "\\|A\\|_\\epsilon = \\max_{\\|x\\|_\\epsilon \\neq 0} \\frac{\\|Ax\\|_\\epsilon}{\\|x\\|_\\epsilon} = \n",
    "\\max_{\\|x\\|_\\epsilon \\neq 0} \\frac{\\|(S D_\\epsilon)^{-1} Ax\\|_\\infty}{\\|(S D_\\epsilon)^{-1} x\\|_\\infty} = \n",
    "\\max_{\\|y\\|_\\infty \\neq 0} \\frac{\\|(S D_\\epsilon)^{-1} A (S D_\\epsilon)y\\|_\\infty}{\\| y\\|_\\infty} = \\\\\n",
    "\\| (S D_\\epsilon)^{-1} A (S D_\\epsilon) \\|_\\infty = \\max_{i=1 \\dots n} |\\lambda_i| + \\epsilon = \\rho(A) + \\epsilon\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema:** Se $A_{m \\times n}$ então $\\lim_{k \\to \\infty} A^k \\to 0 \\iff \\rho(A) < 1$.\n",
    "\n",
    "Ademais, $\\displaystyle \\sum_{k=0}^\\infty A^k$ é convergente $\\iff \\rho(A) < 1$ e nesse caso \n",
    "$$ \\frac{1}{1+ \\|A\\|} \\leq \\| ( I - A)^{-1} \\| \\leq \\frac{1}{1- \\|A\\|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
